{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Twitter Data Set.ipynb","provenance":[],"authorship_tag":"ABX9TyMkLcxvcA2axLVpXeTjwTbS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Import required libraries\n","\n","# Twitter specific\n","import tweepy\n","\n","# General analysis packages\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","import pickle\n","\n","# Language Packages\n","import demoji  \n","from string import punctuation\n","import spacy\n","from spacytextblob.spacytextblob import SpacyTextBlob\n","from textblob import TextBlob\n","from gsdmm import MovieGroupProcess\n","from wordcloud import WordCloud\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","# Preload the spacy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","nlp.add_pipe('spacytextblob') # Allows us to use the textblob sentiment analysis tools with spacy\n","# Add any other required stop words\n","nlp.Defaults.stop_words.add(\"cop26\")\n","\n","# Redirect the current python folder to where the Twitter files are located\n","os.chdir(r'/content/drive/MyDrive/Colab Notebooks/Twitter/') # If you map this to where this notebook is saved then gsdmm shoud install in the same location"],"metadata":{"id":"jXQUM4_m3DOi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download more data"],"metadata":{"id":"2FSknTTC3EK1"}},{"cell_type":"code","source":["# Make sure that Google Drive is mounted before trying to call the below\n","# Loads my private twitter keys\n","%run \"Twitter Info.ipynb\""],"metadata":{"id":"LDzYXR_23U8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up twitter API authentication \n","auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","auth.set_access_token(access_token, access_secret)\n","api = tweepy.API(auth)"],"metadata":{"id":"QkWuTYNX3Wq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9bsrwdD2yB8"},"outputs":[],"source":["# cursor = tweepy.Cursor(api.search, q = 'climate change', lang = 'en', result_type = 'mixed', tweet_mode = 'extended', until = '2021-11-16').items(1)\n","\n","# for i in cursor:\n","#   print(i.full_text)"]},{"cell_type":"code","source":["# # Run a search and save the key variables for later analysis\n","# date_of_run = '2021-11-16' # Should be 7 days in the past\n","# number_of_tweets_query = 17500\n","# tweet = []\n","# likes = []\n","# time = []\n","# is_verified = []\n","# user_name = []\n","# hashtag = []\n","\n","# for tweet_object in tweepy.Cursor(api.search, q = 'climate change', tweet_mode = 'extended', lang = 'en', result_type = 'mixed', until = date_of_run, count = 100).items(number_of_tweets_query):\n","#   tweet.append(tweet_object.full_text)\n","#   likes.append(tweet_object.favorite_count)\n","#   time.append(tweet_object.created_at)\n","#   is_verified.append(tweet_object.user.verified)\n","#   user_name.append(tweet_object.user.name)\n","#   hashtag.append(tweet_object.entities['hashtags'])\n"],"metadata":{"id":"3VExLO6m27tK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = pd.DataFrame({'tweets':tweet,\n","#                    'likes':likes,\n","#                    'time':time,\n","#                    'verified':is_verified,\n","#                    'user':user_name,\n","#                    'hashtags':hashtag})"],"metadata":{"id":"V4KPIp1j27qp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df"],"metadata":{"id":"kDiNVV2T27oY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# file_name = 'df_23_11_21_2.csv'\n","# df.to_csv(r'/content/drive/MyDrive/Colab Notebooks/Twitter/' + file_name, )"],"metadata":{"id":"FdVQQChq27mD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WNSByBMO27jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # *** Here for completeness, the cleaned table has been saved to save time ***\n","# # Merge all the individual data pulls drom twitter into a single table\n","\n","# full_df = pd.concat([pd.read_csv('df_12_11_21.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_12_11_21_2.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_13_11_21_1.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_14_11_21_1.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_14_11_21_2.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_14_11_21_3.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_18_11_21_1.csv', encoding = 'utf-8'),\n","#                     pd.read_csv('df_18_11_21_3.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_20_11_21_1.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_20_11_21_2.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_21_11_21_1.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_21_11_21_2.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_22_11_21_1.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_22_11_21_2.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_23_11_21_1.csv', encoding = 'utf-8'),\n","#                      pd.read_csv('df_23_11_21_2.csv', encoding = 'utf-8')\n","# ])\n","\n","# full_df.drop(columns=('Unnamed: 0'), inplace = True)"],"metadata":{"id":"H4Q-iYQr27gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # *** Here for completeness, the cleaned table has been saved to save time ***\n","# # Remove retweets, remove certain automatic accounts and drop duplicate. Reset index.\n","\n","# tweet_df = full_df\n","# # Filter out retweets\n","# tweet_df['temp'] = full_df.tweets.str[0:2]\n","# tweet_df = full_df[full_df['temp'] != 'RT']\n","# # Filter out useless user info\n","# tweet_df = tweet_df[tweet_df.user != 'üåçüÖ≤üÖæüÖø26 AVIATION MONITOR']\n","# tweet_df = tweet_df[tweet_df.user != 'CentralSpotterüè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø']\n","# # Drop copied / botted tweets\n","# tweet_df = tweet_df.drop_duplicates(subset = ['tweets'])\n","# tweet_df.reset_index(drop = True, inplace = True)\n","# tweet_df.drop(columns = ('temp'), inplace = True)\n","\n","# # Save the combined data to save further time\n","# tweet_df.to_csv('tweet_df_initial_clean.csv', encoding = 'utf-8')"],"metadata":{"id":"NYlVSRrc3Oyi"},"execution_count":null,"outputs":[]}]}